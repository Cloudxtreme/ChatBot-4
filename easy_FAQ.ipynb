{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages and set TEMP_FOLDER to save dictionary, corpus and trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import tempfile\n",
    "import nltk\n",
    "import sys\n",
    "import time\n",
    "from gensim import corpora, models, similarities\n",
    "from six import iteritems\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import *\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "TEMP_FOLDER = tempfile.gettempdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to create dictionary and corpus and train lsi model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dict_and_corpus (q_text, a_text):\n",
    "    #usage: \"questions.txt\", \"answers.txt\", \"dictionary.dict\", \"corpus.mm\"\n",
    "    #Create dictionary\n",
    "    nltk.extract_rels\n",
    "    lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "    stoplist = set(stopwords.words('english'))\n",
    "    #TAG NOT COMPLETE!!!!!\n",
    "    tags=['WRB', 'WP', 'WP$', 'WPT', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'NNP', 'NNPS', 'NN', 'NNS', 'JJ', 'JJS', 'JJ']\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    text_cleaned = []\n",
    "    for q_line, a_line in zip(open(q_text), open(a_text)):\n",
    "        line_cleaned = []\n",
    "        tokens = tokenizer.tokenize(q_line.lower()+a_line.lower())\n",
    "        pos = nltk.pos_tag(tokens)\n",
    "        for i, j in pos:\n",
    "            if (i not in stoplist):\n",
    "                if (j in tags):\n",
    "                    line_cleaned.append(lemma.lemmatize(i))\n",
    "                    line_cleaned.append(i)\n",
    "        text_cleaned.append(line_cleaned)\n",
    "    dictionary=corpora.Dictionary(text_cleaned)\n",
    "    dictionary.compactify()\n",
    "    dictionary.save(os.path.join(TEMP_FOLDER, 'dictionary.dict'))\n",
    "    #Create corpus\n",
    "    class Corpus(object):\n",
    "        def __iter__(self):\n",
    "            for q_line, a_line in zip(open(q_text), open(a_text)):\n",
    "                line = q_line.lower().split()+a_line.lower().split()\n",
    "                yield dictionary.doc2bow(line)\n",
    "    corpus = Corpus()\n",
    "    corpora.MmCorpus.serialize(os.path.join(TEMP_FOLDER, 'corpus.mm'), corpus)\n",
    "    corpus = corpora.MmCorpus(os.path.join(TEMP_FOLDER, 'corpus.mm'))\n",
    "    #Train tf-idf model and lsi model\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=5)\n",
    "    lsi.save(os.path.join(TEMP_FOLDER,'model.lsi')) # \n",
    "    #Create similarities matrix\n",
    "    corpus_lsi = lsi[corpus_tfidf]\n",
    "    index = similarities.MatrixSimilarity(lsi[corpus])\n",
    "    index.save(os.path.join(TEMP_FOLDER, 'index.index'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to process one user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def answer_to_user_query(user_query, dictionary, lsi, index, answer_lines):\n",
    "    #user_query=input(\"How can I help you? \")\n",
    "    lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    #Process query\n",
    "    tokens = [lemma.lemmatize(word) for word in tokenizer.tokenize(user_query)]\n",
    "    vec_bow=dictionary.doc2bow(tokens)\n",
    "    vec_lsi=lsi[vec_bow]\n",
    "    sims = index[vec_lsi]\n",
    "    sims = sorted(enumerate(sims), key=lambda item:-item[1])\n",
    "    match=sims[0][0]\n",
    "    answer = nltk.sent_tokenize(answer_lines[match])\n",
    "    if (len(answer)<3):\n",
    "        for line in answer:\n",
    "            print(line)\n",
    "            time.sleep(len(line)*0.04)\n",
    "    else:\n",
    "        print(answer[0]+'...')\n",
    "        time.sleep(len(answer[0])*0.04)\n",
    "        user_response=input(\"Is this answer relevant?(yes/no)\")\n",
    "        if (user_response=='yes'):\n",
    "            time.sleep(2)\n",
    "            print('Great! Here is the rest of the answer:')\n",
    "            time.sleep(2)\n",
    "            for i in range(1, len(answer)):\n",
    "                print(answer[i])\n",
    "                time.sleep(len(answer[i])*0.04)\n",
    "        else:\n",
    "            print('Sorry. You will need to ask someone else')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate and load the dictionary, corpus, model and index matrix once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Execute only once\n",
    "q_text='questions.txt'\n",
    "a_text='answers.txt'\n",
    "create_dict_and_corpus (q_text, a_text)\n",
    "\n",
    "dictionary = corpora.Dictionary.load(os.path.join(TEMP_FOLDER, 'dictionary.dict'))\n",
    "lsi = models.LsiModel.load(os.path.join(TEMP_FOLDER,'model.lsi'))\n",
    "index=similarities.MatrixSimilarity.load(os.path.join(TEMP_FOLDER, 'index.index'))\n",
    "answer_lines=[line for line in open(a_text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the answer_to_user_query for each new query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can I help you? \n",
      " Hot to add marker?\n",
      "The sprint marker is a Scrum tool that can be used to quickly add multiple issues in the backlog into an inactive sprint....\n",
      "Is this answer relevant?(yes/no)yes\n",
      "Great! Here is the rest of the answer:\n",
      "The sprint marker requires a Sprint to be created first before it becomes visible and ready to be used.\n",
      "(1)In Plan Mode, create a sprint by clicking on the Create Sprint button.\n",
      "(2)You should be able to see the Sprint marker below the inactive sprint.\n",
      "(3)Simply click and drag the marker downwards to automatically include issues from the backlog into this sprint.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Execute for each new query\n",
    "answer_to_user_query(input(\"How can I help you? \\n \"), dictionary, lsi, index, answer_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can I help you? \n",
      "What is it?\n",
      "Sorry, I do not know the answer.\n",
      "\n",
      "How can I help you? \n",
      " How can I set up authorization for scrum board?\n",
      "Custom fields in JIRA (story points is a custom field) can be configured with a context that restricts it's usage to certain projects and/or issue types....\n",
      "Is this answer relevant?(yes/no)no\n",
      "Sorry. You will need to ask someone else\n"
     ]
    }
   ],
   "source": [
    "#Execute for each new query\n",
    "answer_to_user_query(input(\"How can I help you? \\n\"), dictionary, lsi, index, answer_lines)\n",
    "#Execute for each new query\n",
    "answer_to_user_query(input(\"How can I help you? \\n \"), dictionary, lsi, index, answer_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
